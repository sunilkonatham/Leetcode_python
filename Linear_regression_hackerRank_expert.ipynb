{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e36eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "2 7\n",
    "0.18 0.89 109.85\n",
    "1.0 0.26 155.72\n",
    "0.92 0.11 137.66\n",
    "0.07 0.37 76.17\n",
    "0.85 0.16 139.75\n",
    "0.99 0.41 162.6\n",
    "0.87 0.47 151.77\n",
    "4\n",
    "0.49 0.18\n",
    "0.57 0.8\n",
    "0.56 0.64\n",
    "0.76 0.18\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f9c20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "6 15\n",
    "0.87 0.69 0.37 0.22 0.16 0.99 72.38\n",
    "0.24 0.42 0.23 0.75 0.3 0.56 107.39\n",
    "0.55 0.54 0.25 0.44 0.02 0.63 74.76\n",
    "0.39 0.36 0.82 0.99 0.49 0.6 152.03\n",
    "0.4 0.76 0.52 0.76 0.58 0.03 140.92\n",
    "0.2 0.43 0.5 0.04 0.12 0.86 68.63\n",
    "0.74 0.59 0.12 0.65 0.92 0.54 146.89\n",
    "0.36 0.19 0.01 0.03 0.34 0.79 78.62\n",
    "0.38 0.49 0.32 0.34 0.31 0.67 96.13\n",
    "0.07 0.65 0.97 0.03 0.92 0.94 151.32\n",
    "0.68 0.65 0.52 0.03 0.4 0.45 87.63\n",
    "0.26 0.82 0.58 0.07 0.99 0.73 146.44\n",
    "0.29 0.48 0.36 0.29 0.71 0.32 125.81\n",
    "0.46 0.67 0.7 0.79 0.98 0.88 179.37\n",
    "0.61 0.2 0.68 0.51 0.31 0.05 108.08\n",
    "76\n",
    "0.15 0.39 0.06 0.61 0.33 0.09\n",
    "0.54 0.14 0.53 0.01 0.93 0.78\n",
    "0.69 0.74 0.07 0.84 0.13 0.64\n",
    "0.92 0.04 0.24 0.5 0.19 0.34\n",
    "0.82 0.03 0.9 0.87 0.4 0.22\n",
    "0.31 0.21 0.56 0.27 0.83 0.84\n",
    "0.79 0.83 0.64 0.82 0.39 0.46\n",
    "0.58 0.37 0.81 0.34 0.77 0.98\n",
    "0.01 0.9 0.15 0.43 0.9 0.77\n",
    "0.76 0.76 0.49 0.01 0.95 0.14\n",
    "0.69 0.31 0.8 0.08 0.36 0.35\n",
    "0.62 0.63 0.46 0.26 0.01 0.51\n",
    "0.7 0.91 0.95 0.35 0.47 0.59\n",
    "0.16 0.16 0.81 0.02 0.04 0.77\n",
    "0.98 0.85 0.34 0.68 0.74 0.25\n",
    "0.98 0.86 0.08 0.62 0.25 0.42\n",
    "0.76 0.78 0.61 0.03 0.02 0.65\n",
    "0.5 0.07 0.08 0.8 0.23 0.28\n",
    "0.13 0.42 0.19 0.52 0.94 0.44\n",
    "0.03 0.89 0.03 0.52 0.37 0.21\n",
    "0.68 0.5 0.92 0.9 0.66 0.79\n",
    "0.74 0.7 0.21 0.88 0.4 0.07\n",
    "0.37 0.8 0.6 0.65 0.86 0.95\n",
    "0.2 0.32 0.61 0.55 0.52 0.61\n",
    "0.61 0.93 0.49 0.37 0.59 0.63\n",
    "0.3 0.07 0.55 0.2 0.72 0.46\n",
    "0.52 0.74 0.39 0.12 0.16 0.7\n",
    "0.46 0.82 0.21 0.27 0.2 0.36\n",
    "0.14 0.48 0.61 0.19 0.81 0.99\n",
    "0.75 0.93 0.04 0.32 0.65 0.09\n",
    "0.72 0.75 0.82 0.41 0.17 0.61\n",
    "0.4 0.92 0.91 0.57 0.47 0.13\n",
    "0.3 0.9 0.8 0.83 0.25 0.51\n",
    "0.25 0.61 0.49 0.51 0.18 0.75\n",
    "0.72 0.82 0.94 0.3 0.91 0.21\n",
    "0.41 0.49 0.59 0.99 0.58 0.8\n",
    "0.32 0.25 0.48 0.32 0.38 0.97\n",
    "0.7 0.61 0.65 0.21 0.31 0.69\n",
    "0.8 0.48 0.11 0.57 0.26 0.95\n",
    "0.84 0.22 0.27 0.74 0.26 0.74\n",
    "0.81 0.61 0.35 0.35 0.43 0.15\n",
    "0.57 0.36 0.28 0.65 0.97 0.66\n",
    "0.45 0.43 0.7 0.72 0.03 0.19\n",
    "0.59 0.09 0.9 0.61 0.89 0.38\n",
    "0.9 0.66 0.5 0.37 0.27 0.12\n",
    "0.4 0.28 0.76 0.67 0.89 0.51\n",
    "0.5 0.91 0.12 0.32 0.16 0.79\n",
    "0.64 0.96 0.27 0.18 0.11 0.55\n",
    "0.32 0.79 0.7 0.03 0.87 0.42\n",
    "0.82 0.2 0.48 0.07 0.11 0.52\n",
    "0.38 0.77 0.88 0.68 0.41 0.64\n",
    "0.69 0.03 0.2 0.91 0.85 0.26\n",
    "0.34 0.15 0.73 0.64 0.39 0.52\n",
    "0.97 0.25 0.12 0.1 0.08 0.65\n",
    "0.1 0.96 0.71 0.0 0.37 0.19\n",
    "0.42 0.89 0.31 0.34 0.35 0.01\n",
    "0.88 0.78 0.92 0.6 0.3 0.45\n",
    "0.8 0.02 0.32 0.18 0.05 0.1\n",
    "0.53 1.0 0.28 0.8 0.04 0.19\n",
    "0.59 0.02 0.1 0.04 0.56 0.12\n",
    "0.95 0.27 0.86 0.1 0.67 0.14\n",
    "0.76 0.43 0.74 0.17 0.89 0.31\n",
    "0.81 0.33 0.85 0.17 0.21 0.56\n",
    "0.72 1.0 0.7 0.46 0.79 0.2\n",
    "1.0 0.67 0.05 0.63 0.68 0.26\n",
    "0.05 0.83 0.75 0.53 0.5 0.11\n",
    "0.4 0.56 0.49 0.99 0.02 0.88\n",
    "0.04 0.23 0.15 0.47 0.73 0.7\n",
    "0.7 0.25 0.77 0.99 0.65 0.09\n",
    "0.23 0.22 0.56 0.18 0.43 0.6\n",
    "0.86 0.05 0.61 0.03 0.75 0.04\n",
    "0.27 0.58 0.83 0.56 0.43 0.86\n",
    "0.94 0.17 0.66 0.85 0.06 0.62\n",
    "0.56 0.03 0.32 0.99 0.79 0.69\n",
    "0.86 0.03 0.29 0.17 0.57 0.44\n",
    "0.43 0.16 0.93 0.82 0.25 0.59\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa14fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 74.25905049 151.36716515  62.05372687  87.29673705 152.12386879\n",
      " 151.28642899 115.10456798 164.99979476 107.50170231 123.32617285\n",
      " 133.83856478  85.46301895 142.0112353  132.26084934 102.46689156\n",
      "  59.24307093  94.11923103  79.72835363 118.9019899   64.07021882\n",
      " 164.95528891  79.12435669 145.23649248 136.44138605 111.84292148\n",
      " 142.50689892  87.36960774  69.30015622 150.92726048  69.35493824\n",
      " 121.02106072 135.37755399 123.93042984 104.51375425 159.1007684\n",
      " 137.41545795 123.76948946 117.78368123  80.0455183   96.7028148\n",
      "  91.63732376 129.64039178 109.76238418 177.52853056  93.59423338\n",
      " 165.43363087  64.2014832   67.23254748 143.12829247  99.79431542\n",
      " 141.80561772 119.84900518 141.84668227  69.07975773 113.32849557\n",
      "  78.67743072 132.29085871  82.88481309  62.72721092  91.4898029\n",
      " 149.99012254 151.73111416 132.63501146 130.3365859   80.16852585\n",
      " 128.29163187 101.27763963 114.03968116 148.4762619  127.87670515\n",
      " 137.90123949 146.67428164 117.49383191 134.27347311 110.38054649\n",
      " 152.13806418]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sys.stdin = StringIO(text1)\n",
    "n = sys.stdin.readline().split()\n",
    "if not n:\n",
    "    n = sys.stdin.readline().split()\n",
    "\n",
    "for i in range(len(n)):\n",
    "    n[i] = int(n[i])\n",
    "\n",
    "data_dict = {}\n",
    "for i in range(n[0]+1):\n",
    "    data_dict['x'+str(i)] = []\n",
    "    \n",
    "for j in range(n[1]):\n",
    "    lines = sys.stdin.readline().split()\n",
    "    for i in range(n[0]+1):\n",
    "        data_dict['x'+str(i)].append(float(lines[i]))\n",
    "\n",
    "df = pd.DataFrame(data_dict)        \n",
    "x = df.drop(df.columns[-1], axis=1) #X_train\n",
    "y = df[df.columns[-1]]              #Y_train\n",
    "\n",
    "n_test = int(sys.stdin.readline().split()[0])\n",
    "\n",
    "data_dict_test = {}\n",
    "for i in range(n[0]):\n",
    "    data_dict_test['x'+str(i)] = []\n",
    "    \n",
    "for j in range(n_test):\n",
    "    lines = sys.stdin.readline().split()\n",
    "    for i in range(n[0]):\n",
    "        data_dict_test['x'+str(i)].append(float(lines[i]))\n",
    "        \n",
    "x_test = pd.DataFrame(data_dict_test)    #X_test\n",
    "\n",
    "# As the above variables has have multicolinearity, we will apply PCA\n",
    "def apply_pca(x):\n",
    "    col = []\n",
    "    n_comp = len(x.columns)\n",
    "    \n",
    "    #applying standard scalar before PCA\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    #Applying PCA in for floop\n",
    "    for i in range(1,n_comp):\n",
    "        n_components=i\n",
    "        pca = PCA(n_components=i)\n",
    "        p_comp = pca.fit_transform(x)\n",
    "        evr = np.cumsum(pca.explained_variance_ratio_)\n",
    "        if evr[i-1]>0.9:\n",
    "            n_components = i\n",
    "            break\n",
    "    \n",
    "    \n",
    "    #creating data frame\n",
    "    for j in range(1,n_components+1):\n",
    "        col.append('pc'+str(j))\n",
    "        \n",
    "    result = pd.DataFrame(data = p_comp, columns=col)\n",
    "    return result\n",
    "\n",
    "x = apply_pca(df.drop(df.columns[-1], axis=1))\n",
    "y = df[df.columns[-1]]\n",
    "pca_df_test = apply_pca(x_test)\n",
    "\n",
    "#x = pca_df.drop(pca_df.columns[-1], axis=1)\n",
    "#y = pca_df[pca_df.columns[-1]]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "print(lr.predict(pca_df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0512f8",
   "metadata": {},
   "source": [
    "# Submitted in Hacker rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464dcbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "n = sys.stdin.readline().split()\n",
    "if not n:\n",
    "    n = sys.stdin.readline().split()\n",
    "\n",
    "for i in range(len(n)):\n",
    "    n[i] = int(n[i])\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "for i in range(n[0]+1):\n",
    "    data_dict['x'+str(i)] = []\n",
    "    \n",
    "for j in range(n[1]):\n",
    "    lines = sys.stdin.readline().split()\n",
    "    for i in range(n[0]+1):\n",
    "        data_dict['x'+str(i)].append(float(lines[i]))\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "x = df.drop([df.columns[-1]], axis=1)\n",
    "y = df[[df.columns[-1]]]\n",
    "\n",
    "n_test = int(sys.stdin.readline().split()[0])\n",
    "\n",
    "data_dict_test = {}\n",
    "for i in range(n[0]):\n",
    "    data_dict_test['x'+str(i)] = []\n",
    "    \n",
    "for j in range(n_test):\n",
    "    lines = sys.stdin.readline().split()\n",
    "    for i in range(n[0]):\n",
    "        data_dict_test['x'+str(i)].append(float(lines[i]))\n",
    "        \n",
    "x_test = pd.DataFrame(data_dict_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "lr_pred = lr.predict(x_test)\n",
    "\n",
    "for i in lr_pred:\n",
    "    print(f\"{i[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9200306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228231a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0f347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9096b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c54e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ef340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f41fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120fdb85",
   "metadata": {},
   "source": [
    "# Things to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the descriptive statistics to get the better understanding of the data\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def custom_summary(df):\n",
    "\n",
    "    result =[]\n",
    "    \n",
    "    for col in list(df.columns):\n",
    "        \n",
    "        #Looking at the statistical parameters\n",
    "        \n",
    "        stats = OrderedDict({\"featureName\":col,\n",
    "                            \"count\":df[col].count(),\n",
    "                            \"datatype\":df[col].dtype,\n",
    "                            \"min\":df[col].min(),\n",
    "                            \"Q1\":df[col].quantile(0.25),\n",
    "                            \"Q2\":df[col].quantile(0.5),\n",
    "                            \"Q3\":df[col].quantile(0.75),\n",
    "                            \"Q4\":df[col].quantile(1.0),\n",
    "                            \"mean\":df[col].mean(),\n",
    "                            \"stdv\":df[col].std(),\n",
    "                            \"var\":df[col].var(),\n",
    "                            \"kurt\":df[col].kurt(),\n",
    "                            \"skew\":df[col].skew(),\n",
    "                            \"range\":df[col].max() - df[col].min(),\n",
    "                            \"IQR\": df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "                            })\n",
    "        \n",
    "        # Checking the skewness of the data\n",
    "        \n",
    "        if df[col].skew()<-1.0:\n",
    "            label = \"Highly negatively skewed\"\n",
    "        elif -1.0<df[col].skew()<-0.5:\n",
    "            label = \"moderately negatively skewed\"\n",
    "        elif -0.5<df[col].skew()<0.5:\n",
    "            label = \"fairly symmetric\"\n",
    "        elif 0.5<df[col].skew()<1.0:\n",
    "            label = \"moderately positively skewed\"\n",
    "        else:\n",
    "            label = \"Highly positively skewed\"\n",
    "            \n",
    "        stats['skewness comment'] = label;\n",
    "        \n",
    "        \n",
    "        #Outliers identification\n",
    "        \n",
    "        upper_limit = stats['Q3'] + (1.5*stats['IQR'])\n",
    "        lower_limit = stats['Q1'] - (1.5*stats['IQR'])\n",
    "        if len([x for x in df[col] if x <lower_limit or x>upper_limit ]) > 0:\n",
    "            outliers_label = \"Has outliers\"\n",
    "        else:\n",
    "            outliers_label = \"No outliers\"\n",
    "        \n",
    "        stats['Outlier comment'] = outliers_label        \n",
    "        stats['number of outliers'] = len([x for x in df[col] if x <lower_limit or x>upper_limit ])\n",
    "        \n",
    "        #Calculating the outliers percentage\n",
    "        \n",
    "        stats['Percentage of outliers'] = stats['number of outliers']*100/stats['count']\n",
    "        \n",
    "        result.append(stats)\n",
    "    resultdf = pd.DataFrame(data=result)\n",
    "    return resultdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55589c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treating the outliers using outlier tretment function\n",
    "\n",
    "def otf(data, col, method = 'Quartile', strategy= 'Median'):\n",
    "    coldata = data[col]\n",
    "    \n",
    "    #Using Quartile method to identify the outliers\n",
    "    if method == 'Quartile':\n",
    "        median = coldata.median()\n",
    "        Q1 = coldata.quantile(0.25)\n",
    "        Q3 = coldata.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        upper_limit = Q3 + (1.5*IQR)\n",
    "        lower_limit = Q1 - (1.5*IQR)\n",
    "        \n",
    "    #Using Standard deviation method to identify the outliers    \n",
    "    elif method == 'std':\n",
    "        colmean = coldata.mean()\n",
    "        colstd = coldata.std()\n",
    "        upper_limit = colmean + (2*colstd)\n",
    "        lower_limit = colmean - (2*colstd)\n",
    "        \n",
    "    else:\n",
    "        print('Invalid method')\n",
    "             \n",
    "    #Identifying the outliers        \n",
    "    outlier = data.loc[(coldata < lower_limit) | (coldata > upper_limit), col]\n",
    "    \n",
    "    #Calculating outlier percentage\n",
    "    outlier_percentage = round(len(outlier)*100/len(data), 2)\n",
    "    \n",
    "    \n",
    "    if len(outlier) == 0:\n",
    "        print(f\"{col} doesn't have any outliers\")\n",
    "    else:\n",
    "        print(f\"Total number of outliers in {col} is {len(outlier)}\")\n",
    "        print(f'Percentage of outliers in {col} data is {outlier_percentage}')\n",
    "    \n",
    "    #Using median and mean strategy to replace outliers\n",
    "    if strategy == 'Median':\n",
    "        data.loc[(coldata < lower_limit) | (coldata > upper_limit), col] = median\n",
    "    elif strategy == \"Mean\":\n",
    "        data.loc[(coldata < lower_limit) | (coldata > upper_limit), col] = colmean\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sc\n",
    "#Building ODT plots to understand outliers distribution\n",
    "\n",
    "def odt_plots(df, col):\n",
    "    fig,(ax1, ax2, ax3) = plt.subplots(1,3,figsize = (16,5))\n",
    "    \n",
    "    kwargs = {'fontsize':15, 'color':'black'}    #key word arguements\n",
    "    \n",
    "    #box plot with outliers\n",
    "    sns.boxplot(df[col],ax=ax1,color = 'b')\n",
    "    ax1.set_title('Box plot for '+col, **kwargs)\n",
    "    ax1.set_xlabel('values', **kwargs)\n",
    "    ax1.set_ylabel('Box distribution', **kwargs)\n",
    "    \n",
    "    #histogram with outliers\n",
    "    sns.distplot(df[col],ax=ax2,color = 'r', fit=sc.norm)\n",
    "    ax2.set_title('Histogram plot with outliers for '+col, **kwargs)\n",
    "    ax2.set_xlabel('values', **kwargs)\n",
    "    ax2.set_ylabel('Histrogram', **kwargs)\n",
    "    \n",
    "    #histogram with no outliers\n",
    "    y = otf(df,col)\n",
    "    sns.distplot(y[col], ax = ax3, color = 'y', fit=sc.norm)\n",
    "    ax3.set_title('Histogram plot without outliers for '+col, **kwargs)\n",
    "    ax3.set_xlabel('values', **kwargs)\n",
    "    ax3.set_ylabel('Histrogram', **kwargs)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52309453",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    odt_plots(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the relationship between the dependent and the independent variables (Correlation) (with regression plots)\n",
    "for col in list(df.columns):\n",
    "    if col != 'x6':\n",
    "        f,ax1 = plt.subplots(figsize = (10,10))\n",
    "        sns.regplot(x = df[col], y=df['x6'], ax=ax1).set_title(f'Relationship between {col} and price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding out the correlation with the target (correlation matrix)\n",
    "def corr_variables(data, target):\n",
    "    ind_var = data.drop([target], axis=1).columns\n",
    "    corr_res =[]\n",
    "    for col in ind_var:\n",
    "        corr_res.append(data[target].corr(data[col]))\n",
    "    res_df = pd.DataFrame([ind_var, corr_res], index=['variable','correlation']).T.sort_values('correlation', ascending = False)\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2cc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_variables(df, 'x6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(abs(x.corr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "def vif_colinearity(data, target):\n",
    "    ind_var = data.drop([target], axis=1)\n",
    "    vif = pd.DataFrame()\n",
    "    vif['vif factor'] = [variance_inflation_factor(ind_var.values,i) for i in range(ind_var.shape[1])]\n",
    "    vif['variable'] = ind_var.columns\n",
    "    \n",
    "    return vif.sort_values('vif factor', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14716feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_colinearity(df, 'x6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the above variables has have multicolinearity, we will apply PCA\n",
    "def apply_pca(x):\n",
    "    col = []\n",
    "    n_comp = len(x.columns)\n",
    "    \n",
    "    #applying standard scalar before PCA\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    #Applying PCA in for floop\n",
    "    for i in range(1,n_comp):\n",
    "        n_components=i\n",
    "        pca = PCA(n_components=i)\n",
    "        p_comp = pca.fit_transform(x)\n",
    "        evr = np.cumsum(pca.explained_variance_ratio_)\n",
    "        if evr[i-1]>0.9:\n",
    "            n_components = i\n",
    "            break\n",
    "    \n",
    "    print(\"The explained variance ratio is: \", evr)\n",
    "    \n",
    "    #creating data frame\n",
    "    for j in range(1,n_components+1):\n",
    "        col.append('pc'+str(j))\n",
    "        \n",
    "    result = pd.DataFrame(data = p_comp, columns=col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = apply_pca(df.drop('x6', axis=1))\n",
    "pca_df = pca_df.join(df[['x6']], how='left')\n",
    "pca_df_test = apply_pca(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08217d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_colinearity(pca_df, 'x6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c601c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name, estimator, data, test_data):\n",
    "    #x_train, x_test, y_train, y_test = train_n_test_split(data, target)\n",
    "    x = data.drop(data.columns[-1], axis=1)\n",
    "    y = data[data.columns[-1]]\n",
    "\n",
    "    estimator.fit(x, y)\n",
    "    \n",
    "    y_pred = estimator.predict(test_data)\n",
    "#    rmsc = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#    r2score = r2_score(y_test, y_pred)\n",
    "    \n",
    "#    temp = [model_name, rmsc, r2score]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2569fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model('Linear regression', LinearRegression(), pca_df,pca_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "#importing data handling libraries\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "#importing data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#importing statistical libraries\n",
    "import scipy.stats as sc\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#importing data preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "#importing machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "#importing validation and model selection libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "#importing libraries to handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71136b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_models(data, target):\n",
    "    \n",
    "    #column names in empty data frame\n",
    "    col_names = ['Model name', 'rmsc', 'r2score']\n",
    "    \n",
    "    #Creating a empty data frame\n",
    "    resultdf = pd.DataFrame(columns=col_names)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('Linear regression', LinearRegression(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('Lasso', Lasso(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('Ridge', Ridge(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('Decision tree', DecisionTreeRegressor(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('KNN', KNeighborsRegressor(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('SVM regressor', SVR(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('randon forests', RandomForestRegressor(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('Ada boost', AdaBoostRegressor(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('Gboost', GradientBoostingRegressor(), data, target)\n",
    "    \n",
    "    resultdf.loc[len(resultdf)] = build_model('xGboost', XGBRegressor(), data, target)\n",
    "    \n",
    "    return resultdf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_models(pca_df, 'x6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031fe76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d158eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235aeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f91815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68156f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbdb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d2db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fca7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
